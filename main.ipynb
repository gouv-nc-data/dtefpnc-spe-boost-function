{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, logging, time\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import google\n",
    "from unidecode import unidecode\n",
    "\n",
    "import functions_framework\n",
    "import google.cloud.logging\n",
    "\n",
    "# BOOST_TOKEN = os.environ['BOOST_TOKEN']\n",
    "BOOST_TOKEN = \"***REMOVED***\"\n",
    "# BOOST_TABLE = \"offre_emploi\" # ancienne table sans dates de modif\n",
    "BOOST_TABLE = \"olap_offre_emploi_log\"\n",
    "BOOST_URL = f\"https://www.province-sud.nc/drhouseweb/api/GOUV_WEB/societe/{BOOST_TABLE}/data?apiKey={BOOST_TOKEN}\"\n",
    "\n",
    "# PROD\n",
    "GCP_PROJECT_ID =\"prj-dtefpnc-p-bq-c3bc\"\n",
    "BQ_DATASET = \"boost\"\n",
    "BQ_TABLE = \"offres_v3\"\n",
    "\n",
    "cred_file = 'prj-dtefpnc-p-bq-c3bc-311cdf7d3088.json'\n",
    "env_local = os.path.exists(cred_file)\n",
    "if env_local:\n",
    "    creds = service_account.Credentials.from_service_account_file(cred_file)\n",
    "else:\n",
    "    creds, _ = google.auth.default()\n",
    "\n",
    "    # Gestion des logs dans cloud run\n",
    "    logging_client = google.cloud.logging.Client()\n",
    "    logging_client.setup_logging()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(GCP_PROJECT_ID, credentials=creds,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_update_date(bq_client):\n",
    "    try:\n",
    "        query = f\"SELECT max(_operation_date) last_update FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\"\n",
    "        query_job = bq_client.query(query)\n",
    "        \n",
    "        for row in query_job.result():\n",
    "            last_update = row.last_update\n",
    "\n",
    "        if last_update:            \n",
    "            # Convertir au format Boost\n",
    "            boost_format_date = last_update.strftime('%Y-%m-%d')\n",
    "            logging.debug(f\"Date formatée pour Boost: {boost_format_date}\")\n",
    "        else:\n",
    "            logging.warning(\"Aucune date récupérée\")\n",
    "            return None\n",
    "\n",
    "        return boost_format_date\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Une erreur s'est produite : {e}\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_commune(nom):\n",
    "    if nom is not None:\n",
    "        return unidecode(nom).lower()\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_offers(start_date,start_nb=0):\n",
    "    data = []\n",
    "\n",
    "    if not start_date:\n",
    "        start_date = \"2000-01-01\"\n",
    "\n",
    "    while True:\n",
    "        params = f\"&_operation_date|datetimeGt={start_date}&_order=_operation_date&start={start_nb}\"\n",
    "        url = BOOST_URL + params\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if response.status_code == 500 and attempt < max_retries:\n",
    "                    logging.warning(f\"Tentative {attempt} échouée, nouvelle tentative dans 3 secondes...\")\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    raise  # Relancer l'exception si on est à la dernière tentative\n",
    "    \n",
    "        try:\n",
    "            res = response.json()\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Erreur lors de l'extraction du JSON : {e}\")\n",
    "            logging.error(f\"Erreur de parse, reponse : {response.text}\")\n",
    "\n",
    "        data.extend(res[\"data\"])\n",
    "\n",
    "    \n",
    "        for offre in res[\"data\"]:\n",
    "            if \"logiciel\" not in offre:\n",
    "                logging.info(url)\n",
    "\n",
    "        if len(data) % 5000 == 0:\n",
    "            logging.info(f'{len(data)} lignes récoltées jusqu\\'à présent.')\n",
    "\n",
    "        if not res[\"hasNextPage\"] or res[\"paramsNextPageQuery\"] is None:\n",
    "            break\n",
    "\n",
    "        start_nb = res[\"paramsNextPageQuery\"][\"start\"]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform(offres):\n",
    "    logging.info(\"######## transform ########\")\n",
    "    for offre in offres:\n",
    "        if isinstance(offre, dict):\n",
    "            del offre[\"logiciel\"]\n",
    "        else:\n",
    "            logging.error(f\"Error: Expected a dict but got : {type(offre)}\")\n",
    "            logging.info(f\"Offre : {offre}\")\n",
    "    return offres\n",
    "\n",
    "\n",
    "def upload(json, client_bq, table_name):\n",
    "    logging.info(\"######## upload ########\")\n",
    "    \n",
    "    table_id = \"%s.%s.%s\" % (GCP_PROJECT_ID, BQ_DATASET, table_name)\n",
    "    table_ref = bigquery.Table(table_id)\n",
    "    table = client_bq.create_table(table_ref, exists_ok=True)\n",
    "    job_config = bigquery.LoadJobConfig(autodetect = True,\n",
    "                                        write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE)    \n",
    "    job = client_bq.load_table_from_json(json, table, job_config=job_config)\n",
    "    job.result()\n",
    "    logging.info(f\"Loaded {job.output_rows} rows into {table_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tables(bq_client, target, source):\n",
    "    delete_query  = f\"\"\"\n",
    "    DELETE FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.{target}`\n",
    "    WHERE id IN (\n",
    "    SELECT id FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.{source}`\n",
    "    )\n",
    "    \"\"\"\n",
    "    bq_client.query(delete_query)\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO `{GCP_PROJECT_ID}.{BQ_DATASET}.{target}`\n",
    "    SELECT * FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.{source}`\n",
    "    \"\"\"\n",
    "    bq_client.query(insert_query)\n",
    "\n",
    "    bq_client.delete_table(f'{GCP_PROJECT_ID}.{BQ_DATASET}.{source}')\n",
    "    logging.info(f\"Table temporaire {source} supprimée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table(bq_client, table_name):\n",
    "    # supprime les lignes pour lesquelles le id n'est pas unique en gardant celle avec la date la plus proche\n",
    "    delete_query = f\"\"\"\n",
    "    DELETE FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.{table_name}`\n",
    "    WHERE STRUCT(id, _operation_date) NOT IN (\n",
    "    SELECT AS STRUCT id, MAX(_operation_date) AS _operation_date\n",
    "    FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.{table_name}`\n",
    "    GROUP BY id\n",
    "    )\n",
    "    \"\"\"\n",
    "    bq_client.query(delete_query)\n",
    "    logging.info(\"Table nettoyée des lignes pas à jour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@functions_framework.http\n",
    "def spe_boost(request):\n",
    "\n",
    "    table_name = BQ_TABLE\n",
    "    boost_format_date = get_last_update_date(client)\n",
    "\n",
    "    if boost_format_date:\n",
    "        initial_table = table_name\n",
    "        table_name = '_tmp_' + table_name\n",
    "\n",
    "    offres = fetch_offers(boost_format_date,start_nb=0)\n",
    "    logging.info(f\"Nouvelles offres : {len(offres)}\")\n",
    "    json = transform(offres)\n",
    "\n",
    "    if not len(json) == 0:\n",
    "        upload(json, client, table_name)\n",
    "        clean_table(client, table_name)\n",
    "        \n",
    "        if boost_format_date:\n",
    "            merge_tables(client, initial_table, table_name)\n",
    "\n",
    "    return \"transfert terminé\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transfert terminé'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spe_boost(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     spe_boost(None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
